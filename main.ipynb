{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c94856-fdd6-42ff-962f-0a8ad874035c",
   "metadata": {},
   "source": [
    "### CPSC 672 Network Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7831d8-5d79-479b-b17d-aaadc83c5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "from random import sample\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import ast\n",
    "from scipy import spatial\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b8e33-d8ae-43bf-b9ce-5ad8cda37714",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following few cells read in our data from a excel spreadsheet and organize them into a few arrays:\n",
    "- stock_symbols contains all the actual stock names\n",
    "- dates contains all the column headings (which are dates) from Feb 2010 to Dec 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd9784af-3c04-4221-8e8b-3ff1ce635bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.read_excel('data/all_data.xlsx', sheet_name='percentChange')\n",
    "df_subset = pd.read_excel('data/subset_data.xlsx',sheet_name='percentChange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "34deefe3-a627-47c2-b133-deb18bbd7295",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_symbols = []\n",
    "for stock in df_total['Stock']:\n",
    "    stock_symbols.append(stock)\n",
    "\n",
    "dates = df_total.columns.values\n",
    "dates = dates[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546907a-6506-4d2a-be69-8dd73ace2373",
   "metadata": {},
   "source": [
    "### Calculating links\n",
    "\n",
    "To get the links between any two stocks, we use cosine similarity to determine how correlated two stocks are. The formula for cosine similarity is as follows:\n",
    "$$ \n",
    "x = {1 - \\frac{u \\cdot v}{||u|| * ||v||}  }\n",
    "$$\n",
    "                            \n",
    "the getSimilarities() functions do just that by comparing every stock, with every other stock, for any input date (column). Note one of the functions calculates this over a 12 month period (Yearly) whereas the other function uses the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4ced1a7b-6f3f-4e1c-9adb-03567312800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilaritiesTotal(dataFrame):\n",
    "    similarities = []\n",
    "    for i in tqdm(range(len(dataFrame))):\n",
    "        for j in range(i+1,len(dataFrame)):\n",
    "            first_stock = dataFrame.iloc[i,2:]\n",
    "            second_stock = dataFrame.iloc[j,2:]\n",
    "            result = 1 - spatial.distance.cosine(first_stock,second_stock)\n",
    "            similarities.append((stock_symbols[i], stock_symbols[j], result))\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "22c88aa6-89ea-4bd7-a1c3-d18a93fade15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilaritiesYearly(date,dataFrame):\n",
    "    similarities = []\n",
    "    elems = len(dataFrame[date])\n",
    "    start_col = (np.where(dates == date)[0][0]) + 2 # plus 2 is for indexing away from stock symbol & sector\n",
    "    end_col = start_col + 12\n",
    "    for i in tqdm(range(elems)):\n",
    "        for j in range(i+1,elems):\n",
    "            first_stock = dataFrame.iloc[i,start_col:end_col]\n",
    "            second_stock = dataFrame.iloc[j,start_col:end_col]\n",
    "            result = 1-spatial.distance.cosine(first_stock,second_stock)\n",
    "            similarities.append((stock_symbols[i], stock_symbols[j], result))\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "adebe59f-f79d-40ba-ae31-348ab49626ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllYears(dataFrame):\n",
    "    i = 0\n",
    "    arr = []\n",
    "    start_date = dates[i]\n",
    "    curr_year = start_date.year\n",
    "    while(curr_year < 2021):\n",
    "        yearly_similarity = getSimilaritiesYearly(start_date,dataFrame)\n",
    "        arr.append((start_date,yearly_similarity))\n",
    "        curr_year+=1\n",
    "        i+=12\n",
    "        start_date = dates[i]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869c832-5a8e-4aa3-b897-948a9990ccb7",
   "metadata": {},
   "source": [
    "### Getting the total and Yearly similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07c3ecb9-9c84-43b8-989f-2deb990060c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63b178667884fe7a885894c645063b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "total_similarities = getSimilaritiesTotal(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "da4a2755-e6a4-43e2-ab02-deb6e5bd5791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2bf88617ba447a9175ff334f424a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5b1c288f854e81b1f72b70e6b9c2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4d2279d1c4412c8ffdf0d317c42401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/spatial/distance.py:620: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc22fae22f64134a575ad44724070e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf904fce227471285089b945621dd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8eb5fbab8744798113d7d555451765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b50d8810ac543c58857c9320b528127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc3383b1bb7422c84238f8d107cbb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e1163d0deb488582a1a0f4af5336ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc23377fe187401ba9e35f5b909a378b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dff6baea7248c684701bdf2fc06dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yearly_similarites = getAllYears(df_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7b2e6-8318-4ae8-bf7b-c94cffe8043a",
   "metadata": {},
   "source": [
    "### Exporting\n",
    "At this point all the stock symbols are exported to csv files, each year has it's own specific set of links. And there is also a csv that contains the total similarity across the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "de33eed6-08d2-416d-bc90-8079348b8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeStocks():\n",
    "    with open('data/stocks.csv','w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"ID\tlabel\"])\n",
    "        for stock in stock_symbols:\n",
    "            writer.writerow([str(stock_symbols.index(stock))+\"\\t\"+stock])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d80b4bcb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def writeTotal(threshold):\n",
    "    with open('data/links_total.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Source\tTarget\tWeight\"])\n",
    "        for link in total_similarities:\n",
    "            first_stock_index = str(stock_symbols.index(link[0]))\n",
    "            second_stock_index = str(stock_symbols.index(link[1]))\n",
    "            link_weight = link[2]\n",
    "            if(abs(link_weight) > threshold):\n",
    "                writer.writerow([first_stock_index+\"\\t\"+second_stock_index+\"\\t\"+str(link_weight)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "343a003f-aeb8-4469-94a7-0b300d979ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeYearly(yearly_calc,threshold):\n",
    "    for year in yearly_calc:\n",
    "        curr_year = year[0].year\n",
    "        links = year[1]\n",
    "        file_name = 'data/links_'+str(curr_year)+'.csv'\n",
    "        with open(file_name, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Source\tTarget\tWeight\"])\n",
    "            for link in links:\n",
    "                first_stock_index = str(stock_symbols.index(link[0]))\n",
    "                second_stock_index = str(stock_symbols.index(link[1]))\n",
    "                link_weight = link[2]\n",
    "                if(abs(link_weight) > threshold):\n",
    "                    writer.writerow([first_stock_index+\"\\t\"+second_stock_index+\"\\t\"+str(link_weight)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2e35b6d6-52fa-416a-ae7a-054eab778c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeStocks()\n",
    "writeTotal(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "120db853-58a9-48f2-a59c-ec3519d5a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeYearly(yearly_similarites,0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310190a-5b5f-4c37-b13a-120da003cb76",
   "metadata": {},
   "source": [
    "### Visualizing the Data\n",
    "Now that the data has been processed and exported into their own csv's , we can begin to do some visualiation and more graph theory related calculations.\n",
    "Also, due to lack of computational resources, we have decided to reduce the number of links to 100,000 . The following two functions take the original csv's and take the strongest 100,000 links and save them. Links who do not make the cut are simply discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8e1897-e16d-4bd8-9dde-fb4a92623c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterYearly(num_of_links):\n",
    "    for i in range(2010,2021):\n",
    "        df_link=pd.read_csv(\"data/links_\"+str(i)+\".csv\",delimiter=\"\\t\")\n",
    "        df_link = df_link.sort_values(by=['Weight'],key=abs,ascending=False,ignore_index=True)\n",
    "        df_link[:num_of_links].to_csv(\"data/Filtered/links_\"+str(i)+\".csv\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2383c6d9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def filterTotal(num_of_links):\n",
    "        df_link=pd.read_csv(\"data/links_total.csv\",delimiter=\"\\t\")\n",
    "        df_link = df_link.sort_values(by=['Weight'],key=abs,ascending=False,ignore_index=True)\n",
    "        df_link[:num_of_links].to_csv(\"data/Filtered/links_total.csv\",sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "126b6edb-904f-4e98-a920-e1256e153c07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filterYearly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [65], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m filterYearly(\u001B[38;5;241m100000\u001B[39m)\n\u001B[0;32m      2\u001B[0m filterTotal(\u001B[38;5;241m100000\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mGraphFromFileYearly\u001B[39m(year):\n",
      "\u001B[1;31mNameError\u001B[0m: name 'filterYearly' is not defined"
     ]
    }
   ],
   "source": [
    "filterYearly(100000)\n",
    "filterTotal(100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def GraphFromFileYearlyAbsWeights(year):\n",
    "    df_link=pd.read_csv(\"data/Filtered/links_\"+str(year)+\".csv\",delimiter=\"\\t\")\n",
    "    G = nx.Graph()\n",
    "    for index,row in df_link.iterrows():\n",
    "        G.add_edge(row['Source'],row['Target'] , weight=abs(row['Weight']))\n",
    "    return G"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5940eabe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def GraphFromFileTotalAbsWeights():\n",
    "    df_link=pd.read_csv(\"data/Filtered/links_total.csv\",delimiter=\"\\t\")\n",
    "    G = nx.Graph()\n",
    "    for index,row in df_link.iterrows():\n",
    "        G.add_edge(row['Source'],row['Target'] , weight=abs(row['Weight']))\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "Yearly_Graphs = []\n",
    "for i in range(2010,2021):\n",
    "    G = GraphFromFileYearlyAbsWeights(i)\n",
    "    Yearly_Graphs.append(G)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "Total_Graph = []\n",
    "Total_Graph.append(GraphFromFileTotalAbsWeights())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "def PlotGraphDegDistribution(Graph_list):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.xlabel(r\"degree $k$\", fontsize=16)\n",
    "    plt.ylabel(r\"$P(k)$\", fontsize=16)\n",
    "    for i in range(len(Graph_list)):\n",
    "        G = Graph_list[i]\n",
    "        N = len(G)\n",
    "        L = G.size()\n",
    "        degrees = [G.degree(node,weight='weight') for node in G]\n",
    "        kmin = min(degrees)\n",
    "        kmax = max(degrees)\n",
    "\n",
    "        # Get 10 logarithmically spaced bins between kmin and kmax\n",
    "        bin_edges = np.logspace(np.log10(kmin), np.log10(kmax), num=10)\n",
    "\n",
    "        # histogram the data into these bins\n",
    "        density, _ = np.histogram(degrees, bins=bin_edges, density=True)\n",
    "\n",
    "\n",
    "        # \"x\" should be midpoint (IN LOG SPACE) of each bin\n",
    "        log_be = np.log10(bin_edges)\n",
    "        x = 10**((log_be[1:] + log_be[:-1])/2)\n",
    "        label = 2010 +i if len(Graph_list) > 1 else \"total\"\n",
    "        plt.loglog(x, density, marker='o', label = label)\n",
    "\n",
    "\n",
    "        # remove right and top boundaries because they're ugly\n",
    "        ax = plt.gca()\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "        # Show the plot\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PlotGraphDegDistribution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [10], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# PlotGraphDegDistribution(Yearly_Graphs)           #for ploting total graph degree distribution\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m PlotGraphDegDistribution(Total_Graph)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'PlotGraphDegDistribution' is not defined"
     ]
    }
   ],
   "source": [
    "# PlotGraphDegDistribution(Yearly_Graphs)           #for ploting total graph degree distribution\n",
    "PlotGraphDegDistribution(Total_Graph)               #for ploting total graph degree distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Function Defintions\n",
    "\n",
    "# Function to plot a degree distribution graph (P(k) vs k graph)\n",
    "def plot_degree_dist(G):\n",
    "\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    kmin = min(degrees)\n",
    "    kmax = max(degrees)\n",
    "\n",
    "    if kmin>0:\n",
    "        bin_edges = np.logspace(np.log10(kmin), np.log10(kmax)+1, num=20)\n",
    "    else:\n",
    "        bin_edges = np.logspace(0, np.log10(kmax)+1, num=20)\n",
    "    density, _ = np.histogram(degrees, bins=bin_edges, density=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "    log_be = np.log10(bin_edges)\n",
    "    x = 10**((log_be[1:] + log_be[:-1])/2)\n",
    "    plt.loglog(x, density, marker='o', linestyle='none')\n",
    "    plt.xlabel(r\"degree $k$\", fontsize=16)\n",
    "    plt.ylabel(r\"$P(k)$\", fontsize=16)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "G = Total_Graph[0]\n",
    "CG = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(CG[0])                 # select the biggest connected component"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Clustering Coefficient:  0.6678847956398628\n",
      "Average Shortest Path:  2.728443762372391\n"
     ]
    }
   ],
   "source": [
    "C = np.mean(list(nx.clustering(G).values()))\n",
    "d = nx.average_shortest_path_length(G,weight='weight' )\n",
    "\n",
    "print(\"Average Clustering Coefficient: \", C)\n",
    "print(\"Average Shortest Path: \", d)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
